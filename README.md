# Annotator AI: System Architecture & Workflow

## 1. Website Flow Overview

Annotator AI is designed to streamline the evaluation of image annotation tasks. The user journey is straightforward: an evaluator uploads a "Ground Truth" (GT) file, which serves as the correct answer key. The system instantly analyzes this file to generate a set of evaluation rules. The user can then upload one or more student submission files and the original images. Upon clicking "Run Evaluation," the system compares each student's work against the GT file using the generated rules, producing a detailed report with scores, feedback, and a visual breakdown of any errors.

The data flow is designed for clarity and efficiency:

```
+------------------+      +-------------------------+      +----------------------+
| 1. User Uploads  |----->| 2. AI Analyzes GT File  |----->| 3. Evaluation Rules  |
| Ground Truth File|      | & Extracts Schema       |      | are Displayed to User|
| (JSON/XML/ZIP)   |      | (Server-Side)           |      | (Editable Pseudocode)|
+------------------+      +-------------------------+      +----------------------+
        |                                                            |
        | (User uploads student files & images)                      |
        V                                                            V
+------------------+      +-------------------------+      +----------------------+
| 4. User Clicks   |----->| 5. System Evaluates     |----->| 6. Results Dashboard |
| "Run Evaluation" |      | Annotations using Rules |      | is Displayed         |
|                  |      | (Client-Side)           |      | (Score, Feedback,   |
|                  |      |                         |      |   Visuals)         |
+------------------+      +-------------------------+      +----------------------+
```

This hybrid approach—using AI to set up the rules and a deterministic engine to execute them—ensures both flexibility and objective, consistent scoring.

---

## 2. GT Upload Process

**Ground Truth (GT)** data is the master reference file containing the perfectly annotated data, created by an expert. It is the "answer key" that all student submissions are measured against.

The upload process is as follows:
1.  **Upload**: The user uploads a single GT file via the "Ground Truth Annotations" input. The system accepts standard formats like **COCO JSON** and **CVAT XML 1.1**. For convenience, a **.ZIP archive** containing both the annotation file and the associated images can also be uploaded.
2.  **Extraction (if ZIP)**: If a ZIP file is provided, the system opens it in the browser, identifies the annotation file (`.json` or `.xml`), and extracts all image files, creating local URLs for them to be displayed later.
3.  **Parsing & Validation**: The content of the annotation file is read as plain text. The system checks if it is XML or JSON and uses the appropriate parser (`DOMParser` for XML, `JSON.parse` for JSON) to convert it into a standardized internal data structure. This ensures the rest of the application can handle the data in a uniform way. If parsing fails, an error is shown to the user.
4.  **Schema Generation**: The validated text content of the GT file is securely sent to a server-side AI model (Genkit) to generate the evaluation rules (the "Eval Schema").
5.  **Storage**: All uploaded data (file content, extracted images, and the generated schema) is held temporarily in the application's **client-side state** (in the user's browser). **No user data is permanently stored on any server or database**, ensuring privacy and security. The data is discarded when the page is refreshed.

---

## 3. Code Modification Flow

A key feature of Annotator AI is its ability to **dynamically generate evaluation logic without modifying the core application code.** This is a crucial distinction: the application's underlying code is static, secure, and does not change at runtime.

Instead, it modifies the *parameters* for the evaluation. Here's how it works:

1.  **Dynamic Rule Generation**: When the GT file is uploaded, its content is sent to an AI model. The model's sole purpose is to analyze the structure of the data and return a configuration object, called an `EvalSchema`. This object contains the labels, attributes, and a human-readable pseudocode representation of the evaluation logic.
2.  **User-Editable Logic**: The generated pseudocode is displayed in a text box. A user can **manually edit this pseudocode** to adjust the evaluation logic before running it. For instance, they could remove an attribute from being checked.
3.  **In-Memory Changes**: These changes, whether generated by the AI or edited by the user, exist only in the application's state for the current session. They are temporary and are not saved to any file or database.
4.  **No Code Deployment or Testing Needed**: Because the core evaluation engine is fixed and only its *input parameters* (the `EvalSchema`) change, there is no need for code commits, version control, or staging environments for this process. The system is designed to be safely configured at runtime.
5.  **Rollback**: To revert to the original AI-generated rules after making a manual edit, the user can simply re-upload the Ground Truth file, which will trigger the rule generation process again.

---

## 4. Formula Storage

The "formulas" in Annotator AI are the evaluation rules, which are stored in the `EvalSchema` object. This object is not stored in a traditional database but is managed as follows:

1.  **Generation**: The `EvalSchema` is generated in real-time by the AI model when the GT file is uploaded.
2.  **Storage Location**: It is stored in a **React state variable** within the main page component (`src/app/page.tsx`). This means it resides in the user's browser memory for the duration of their session.
3.  **Retrieval and Application**: When the user clicks "Run Evaluation," the `evaluateAnnotations` function is called. This function receives the current `EvalSchema` object directly from the component's state as an argument. The evaluator then uses the properties of this object to guide its logic.

**Example of the `EvalSchema` Storage Format (JSON):**
This is what the object looks like when stored in the React state.

```json
{
  "labels": [
    {
      "name": "car",
      "attributes": ["license_plate_number", "color"]
    },
    {
      "name": "pedestrian",
      "attributes": []
    }
  ],
  "matchKey": "Annotation No",
  "pseudoCode": "def evaluate_student_annotations(gt, student):\n  # 1. Match annotations using 'Annotation No' as the unique key.\n  # 2. For each matched 'car' annotation:\n  #    a. Calculate IoU for the bounding box.\n  #    b. Check for an exact match on the 'color' attribute.\n  #    c. Check for a text similarity match on the 'license_plate_number' attribute.\n  # 3. For each matched 'pedestrian' annotation:\n  #    a. Calculate IoU for the bounding box.\n  # 4. Identify any annotations present in GT but not in student (missed).\n  # 5. Identify any annotations present in student but not in GT (extra)."
}
```

---

## 5. General Formula Structure

The core evaluation logic is a deterministic algorithm found in `src/lib/evaluator.ts`. It does not rely on AI for the actual scoring. The general "formula" is a multi-step process that computes a final score based on several weighted metrics.

**Final Score Calculation (Pseudocode):**

```
# Weights for each component of the score
detection_weight = 0.40
localization_weight = 0.30
label_weight = 0.20
attribute_weight = 0.10

# 1. Detection Score (F-beta Score) - How well did the student find the objects?
precision = matched_annotations / total_student_annotations
recall = matched_annotations / total_gt_annotations
f_beta_score = (1.25 * precision * recall) / (0.25 * precision + recall)

# 2. Localization Score (IoU) - How well were the bounding boxes placed?
average_iou = sum_of_all_ious / number_of_matched_annotations

# 3. Label Score - How accurately were the objects classified?
label_accuracy = correctly_labeled_annotations / number_of_matched_annotations

# 4. Attribute Score - How accurate was the text in the attributes?
average_attribute_similarity = sum_of_all_attribute_similarities / number_of_attributes_compared

# Final Weighted Score
final_score = (f_beta_score * detection_weight) +
              (average_iou * localization_weight) +
              (label_accuracy * label_weight) +
              (average_attribute_similarity * attribute_weight)

# Result is scaled to a 0-100 score.
```

**Component Breakdown:**
*   **Detection (Precision/Recall)**: Measures whether the student found all the required objects without adding extra, incorrect ones. It's a balance between "not missing anything" and "not adding anything extra."
*   **Localization (IoU)**: Intersection over Union measures how much the student's bounding box overlaps with the expert's box. A score of 1.0 is a perfect overlap.
*   **Label Accuracy**: Checks if the matched object was given the correct category (e.g., correctly identified as a "car" vs. a "truck").
*   **Attribute Accuracy**: For text attributes (like a license plate number), it uses the Levenshtein distance algorithm to calculate a similarity percentage. This allows for minor typos while still penalizing significant errors.

---

## 6. Dynamic Modifications

The "formula" (the evaluation process described above) is dynamically modified by the `EvalSchema` object that is generated from the GT file. The core algorithm remains the same, but the schema tells it *what* to focus on.

**Key Dynamic Parts:**
1.  **`matchKey`**: This is the most critical dynamic modification.
    *   **If the GT file has a unique ID for each annotation (e.g., "Annotation No" or "track_id")**, the AI will identify it and set it as the `matchKey`. The evaluator will then use this key for a perfect 1-to-1 match, which is highly reliable.
    *   **If no such key exists**, the `matchKey` property is omitted. The evaluator then falls back to a different strategy: matching annotations based on a combination of their IoU overlap and whether their labels are the same.
2.  **`labels` and `attributes`**: The schema's list of labels and their associated attributes tells the evaluator which text fields to compare.
    *   **Example (Before)**: If the `EvalSchema` only lists `{"name": "car", "attributes": []}`, the evaluator will only check the IoU and label for cars.
    *   **Example (After)**: If a new GT file is uploaded that includes license plates, the schema will change to `{"name": "car", "attributes": ["license_plate_number"]}`. The evaluator, reading this new schema, will now *also* perform a text similarity check on the `license_plate_number` attribute for any matched cars.

**API/IO Example for Rule Generation:**

This demonstrates how the system's internal API call to the AI model generates the dynamic rules.

**Input (sent to the Genkit `extractEvalSchema` flow):**
```json
{
  "gtFileContent": "{\"images\":[...], \"categories\":[{\"id\":1, \"name\":\"vehicle\"}], \"annotations\":[{\"image_id\":1, \"category_id\":1, \"bbox\":[...], \"attributes\":{\"color\":\"red\", \"Annotation No\":\"123\"}}]}"
}
```

**Output (returned by the AI and stored in state):**
```json
{
  "labels": [
    {
      "name": "vehicle",
      "attributes": ["color", "Annotation No"]
    }
  ],
  "matchKey": "Annotation No",
  "pseudoCode": "def evaluate_student_annotations(...):\n  # 1. Match annotations using 'Annotation No' as the unique key.\n  # 2. For each matched 'vehicle' annotation:\n  #    a. Calculate IoU for the bounding box.\n  #    b. Check for an exact match on the 'color' attribute."
}
```

This output `EvalSchema` is then passed as a parameter to the evaluation engine, effectively "modifying" the formula for this specific run.
